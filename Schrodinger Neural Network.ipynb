{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train loss:  0.4440678 Valid loss:  0.4343995\n",
      "100 Train loss:  0.11149939 Valid loss:  0.124380775\n",
      "200 Train loss:  0.07451732 Valid loss:  0.10803318\n",
      "300 Train loss:  0.048124477 Valid loss:  0.09010908\n",
      "400 Train loss:  0.029711232 Valid loss:  0.080451034\n",
      "500 Train loss:  0.030314192 Valid loss:  0.075510435\n",
      "600 Train loss:  0.023939198 Valid loss:  0.07500907\n",
      "700 Train loss:  0.017582474 Valid loss:  0.057471324\n",
      "800 Train loss:  0.016281148 Valid loss:  0.0630246\n",
      "900 Train loss:  0.014953014 Valid loss:  0.06083543\n",
      "1000 Train loss:  0.013967542 Valid loss:  0.059987385\n",
      "1100 Train loss:  0.010891441 Valid loss:  0.049621657\n",
      "1200 Train loss:  0.010724099 Valid loss:  0.050272938\n",
      "1300 Train loss:  0.010503332 Valid loss:  0.052171074\n",
      "1400 Train loss:  0.009837647 Valid loss:  0.04999168\n",
      "1500 Train loss:  0.009747493 Valid loss:  0.048414174\n",
      "1600 Train loss:  0.009648587 Valid loss:  0.04680336\n",
      "1700 Train loss:  0.009297052 Valid loss:  0.044409502\n",
      "1800 Train loss:  0.008952156 Valid loss:  0.043977316\n",
      "1900 Train loss:  0.008711382 Valid loss:  0.043677993\n",
      "2000 Train loss:  0.0089791585 Valid loss:  0.049183518\n",
      "2100 Train loss:  0.008260948 Valid loss:  0.043316226\n",
      "2200 Train loss:  0.008074144 Valid loss:  0.043146785\n",
      "2300 Train loss:  0.007998478 Valid loss:  0.049407914\n",
      "2400 Train loss:  0.007626204 Valid loss:  0.050825674\n",
      "2500 Train loss:  0.0067856438 Valid loss:  0.047571912\n",
      "2600 Train loss:  0.0059056245 Valid loss:  0.04429938\n",
      "2700 Train loss:  0.005726073 Valid loss:  0.044000126\n",
      "2800 Train loss:  0.0055653686 Valid loss:  0.043879494\n",
      "2900 Train loss:  0.0054168073 Valid loss:  0.043929845\n",
      "3000 Train loss:  0.005441313 Valid loss:  0.044938177\n",
      "3100 Train loss:  0.0053224536 Valid loss:  0.045048296\n",
      "3200 Train loss:  0.005205197 Valid loss:  0.04501362\n",
      "3300 Train loss:  0.005090261 Valid loss:  0.044981614\n",
      "3400 Train loss:  0.004977969 Valid loss:  0.044982933\n",
      "3500 Train loss:  0.0048690173 Valid loss:  0.04500373\n",
      "3600 Train loss:  0.0047636083 Valid loss:  0.045040067\n",
      "3700 Train loss:  0.004661006 Valid loss:  0.045100406\n",
      "3800 Train loss:  0.004566089 Valid loss:  0.045230877\n",
      "3900 Train loss:  0.0044893944 Valid loss:  0.04559656\n",
      "4000 Train loss:  0.004849811 Valid loss:  0.045199055\n",
      "4100 Train loss:  0.004373029 Valid loss:  0.042150654\n",
      "4200 Train loss:  0.0042698607 Valid loss:  0.044529215\n",
      "4300 Train loss:  0.004241812 Valid loss:  0.045878623\n",
      "4400 Train loss:  0.0047040316 Valid loss:  0.048301157\n",
      "4500 Train loss:  0.0047698147 Valid loss:  0.049356338\n",
      "4600 Train loss:  0.004389322 Valid loss:  0.04664906\n",
      "4700 Train loss:  0.0043356405 Valid loss:  0.047962483\n",
      "4800 Train loss:  0.0042057843 Valid loss:  0.045904815\n",
      "4900 Train loss:  0.004009289 Valid loss:  0.04696258\n",
      "5000 Train loss:  0.0039763995 Valid loss:  0.04484171\n",
      "5100 Train loss:  0.003732248 Valid loss:  0.046064597\n",
      "5200 Train loss:  0.0037096872 Valid loss:  0.0435519\n",
      "5300 Train loss:  0.0035937931 Valid loss:  0.04580518\n",
      "5400 Train loss:  0.0039927415 Valid loss:  0.04357862\n",
      "5500 Train loss:  0.003909884 Valid loss:  0.047615502\n",
      "5600 Train loss:  0.0030219886 Valid loss:  0.0432841\n",
      "5700 Train loss:  0.0029681143 Valid loss:  0.043191027\n",
      "5800 Train loss:  0.0029221792 Valid loss:  0.04317687\n",
      "5900 Train loss:  0.00287963 Valid loss:  0.04318979\n",
      "6000 Train loss:  0.0028391464 Valid loss:  0.043218944\n",
      "6100 Train loss:  0.002800215 Valid loss:  0.043257464\n",
      "6200 Train loss:  0.0027626888 Valid loss:  0.0432988\n",
      "6300 Train loss:  0.00272638 Valid loss:  0.043341327\n",
      "6400 Train loss:  0.0026911153 Valid loss:  0.043386776\n",
      "6500 Train loss:  0.0026566708 Valid loss:  0.04343686\n",
      "6600 Train loss:  0.0026228826 Valid loss:  0.04348306\n",
      "6700 Train loss:  0.0025905562 Valid loss:  0.043474622\n",
      "6800 Train loss:  0.0026253122 Valid loss:  0.04308926\n",
      "6900 Train loss:  0.0025994072 Valid loss:  0.043131612\n",
      "7000 Train loss:  0.0025712552 Valid loss:  0.04318284\n",
      "7100 Train loss:  0.002545029 Valid loss:  0.043230113\n",
      "7200 Train loss:  0.0025197987 Valid loss:  0.04327723\n",
      "7300 Train loss:  0.0024955582 Valid loss:  0.04332459\n",
      "7400 Train loss:  0.0024721373 Valid loss:  0.043373536\n",
      "7500 Train loss:  0.0024491688 Valid loss:  0.043426722\n",
      "7600 Train loss:  0.0024261489 Valid loss:  0.043486163\n",
      "7700 Train loss:  0.0024032365 Valid loss:  0.043549113\n",
      "7800 Train loss:  0.002381504 Valid loss:  0.043608066\n",
      "7900 Train loss:  0.0023603735 Valid loss:  0.043661043\n",
      "8000 Train loss:  0.0023396169 Valid loss:  0.04370748\n",
      "8100 Train loss:  0.0023191532 Valid loss:  0.043749075\n",
      "8200 Train loss:  0.0022988752 Valid loss:  0.043789286\n",
      "8300 Train loss:  0.0022786332 Valid loss:  0.04383331\n",
      "8400 Train loss:  0.002258109 Valid loss:  0.043893248\n",
      "8500 Train loss:  0.002236962 Valid loss:  0.044054728\n",
      "8600 Train loss:  0.0022624026 Valid loss:  0.045432415\n",
      "8700 Train loss:  0.0022692443 Valid loss:  0.043721445\n",
      "8800 Train loss:  0.0021919338 Valid loss:  0.0440536\n",
      "8900 Train loss:  0.0021995625 Valid loss:  0.045130197\n",
      "9000 Train loss:  0.002204905 Valid loss:  0.043889116\n",
      "9100 Train loss:  0.0021345709 Valid loss:  0.044210136\n",
      "9200 Train loss:  0.0021183286 Valid loss:  0.045288462\n",
      "9300 Train loss:  0.0021019783 Valid loss:  0.04424615\n",
      "9400 Train loss:  0.0020751844 Valid loss:  0.044401634\n",
      "9500 Train loss:  0.0020463136 Valid loss:  0.044777136\n",
      "9600 Train loss:  0.0020295521 Valid loss:  0.04447517\n",
      "9700 Train loss:  0.002025114 Valid loss:  0.04459779\n",
      "9800 Train loss:  0.0020310576 Valid loss:  0.044661973\n",
      "9900 Train loss:  0.002007514 Valid loss:  0.044560846\n",
      "10000 Train loss:  0.0019710881 Valid loss:  0.045318343\n",
      "10100 Train loss:  0.0019926713 Valid loss:  0.044541262\n",
      "10200 Train loss:  0.001961191 Valid loss:  0.04505689\n",
      "10300 Train loss:  0.001943155 Valid loss:  0.04473887\n",
      "10400 Train loss:  0.0019336864 Valid loss:  0.04481362\n",
      "10500 Train loss:  0.0019140508 Valid loss:  0.04497367\n",
      "10600 Train loss:  0.0018907412 Valid loss:  0.044827037\n",
      "10700 Train loss:  0.0018619897 Valid loss:  0.045285888\n",
      "10800 Train loss:  0.0018640815 Valid loss:  0.04485883\n",
      "10900 Train loss:  0.0018659171 Valid loss:  0.04562816\n",
      "11000 Train loss:  0.0018635106 Valid loss:  0.044803\n",
      "11100 Train loss:  0.0018497286 Valid loss:  0.045112826\n",
      "11200 Train loss:  0.0018240395 Valid loss:  0.045194056\n",
      "11300 Train loss:  0.0017915315 Valid loss:  0.04503478\n",
      "11400 Train loss:  0.0017720468 Valid loss:  0.045751676\n",
      "11500 Train loss:  0.0018155844 Valid loss:  0.04496049\n",
      "11600 Train loss:  0.0016195255 Valid loss:  0.045963902\n",
      "11700 Train loss:  0.0016077952 Valid loss:  0.045944523\n",
      "11800 Train loss:  0.0015969159 Valid loss:  0.045938823\n",
      "11900 Train loss:  0.001586425 Valid loss:  0.045938775\n",
      "12000 Train loss:  0.0015762008 Valid loss:  0.045942076\n",
      "12100 Train loss:  0.001566186 Valid loss:  0.04594785\n",
      "12200 Train loss:  0.0015563543 Valid loss:  0.04595554\n",
      "12300 Train loss:  0.0015466847 Valid loss:  0.04596485\n",
      "12400 Train loss:  0.001537162 Valid loss:  0.045975726\n",
      "12500 Train loss:  0.0015277755 Valid loss:  0.04598802\n",
      "12600 Train loss:  0.0015185111 Valid loss:  0.046001557\n",
      "12700 Train loss:  0.0015093622 Valid loss:  0.046016023\n",
      "12800 Train loss:  0.0015003235 Valid loss:  0.04603106\n",
      "12900 Train loss:  0.0014913939 Valid loss:  0.0460464\n",
      "13000 Train loss:  0.0014825658 Valid loss:  0.046061736\n",
      "13100 Train loss:  0.0014738416 Valid loss:  0.046077024\n",
      "13200 Train loss:  0.00146521 Valid loss:  0.04609212\n",
      "13300 Train loss:  0.0014566695 Valid loss:  0.04610696\n",
      "13400 Train loss:  0.0014482106 Valid loss:  0.04612158\n",
      "13500 Train loss:  0.0014398261 Valid loss:  0.046135858\n",
      "13600 Train loss:  0.0014315185 Valid loss:  0.046149574\n",
      "13700 Train loss:  0.0014232913 Valid loss:  0.04616267\n",
      "13800 Train loss:  0.0014151527 Valid loss:  0.046175223\n",
      "13900 Train loss:  0.0014071027 Valid loss:  0.046187285\n",
      "14000 Train loss:  0.0013991384 Valid loss:  0.04619904\n",
      "14100 Train loss:  0.0013912646 Valid loss:  0.046210397\n",
      "14200 Train loss:  0.0013834715 Valid loss:  0.046221487\n",
      "14300 Train loss:  0.0013757565 Valid loss:  0.046232328\n",
      "14400 Train loss:  0.001368118 Valid loss:  0.046242926\n",
      "14500 Train loss:  0.0013605551 Valid loss:  0.04625333\n",
      "14600 Train loss:  0.0013530651 Valid loss:  0.046263512\n",
      "14700 Train loss:  0.0013456475 Valid loss:  0.046273395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14800 Train loss:  0.0013382959 Valid loss:  0.04628309\n",
      "14900 Train loss:  0.0013310167 Valid loss:  0.046292547\n",
      "15000 Train loss:  0.0013238022 Valid loss:  0.046301663\n",
      "15100 Train loss:  0.0013166541 Valid loss:  0.046310507\n",
      "15200 Train loss:  0.0013095697 Valid loss:  0.046319053\n",
      "15300 Train loss:  0.0013025514 Valid loss:  0.046327293\n",
      "15400 Train loss:  0.0012955966 Valid loss:  0.046335198\n",
      "15500 Train loss:  0.0012887046 Valid loss:  0.046342786\n",
      "15600 Train loss:  0.0012818743 Valid loss:  0.046350107\n",
      "15700 Train loss:  0.0012751044 Valid loss:  0.04635712\n",
      "15800 Train loss:  0.0012683951 Valid loss:  0.046363972\n",
      "15900 Train loss:  0.0012617457 Valid loss:  0.046370532\n",
      "16000 Train loss:  0.0012551539 Valid loss:  0.04637694\n",
      "16100 Train loss:  0.0012486217 Valid loss:  0.046383217\n",
      "16200 Train loss:  0.0012421443 Valid loss:  0.046389267\n",
      "16300 Train loss:  0.0012357238 Valid loss:  0.04639483\n",
      "16400 Train loss:  0.0012293659 Valid loss:  0.046394307\n",
      "16500 Train loss:  0.0012257211 Valid loss:  0.046285473\n",
      "16600 Train loss:  0.0012530101 Valid loss:  0.04599993\n",
      "16700 Train loss:  0.0012257108 Valid loss:  0.04615424\n",
      "16800 Train loss:  0.0012238802 Valid loss:  0.04612526\n",
      "16900 Train loss:  0.0012178299 Valid loss:  0.046133343\n",
      "17000 Train loss:  0.0012123083 Valid loss:  0.04613689\n",
      "17100 Train loss:  0.001207341 Valid loss:  0.04613618\n",
      "17200 Train loss:  0.0012021819 Valid loss:  0.04613681\n",
      "17300 Train loss:  0.0011971516 Valid loss:  0.046136413\n",
      "17400 Train loss:  0.0011921412 Valid loss:  0.046136227\n",
      "17500 Train loss:  0.0011871635 Valid loss:  0.046136163\n",
      "17600 Train loss:  0.001182216 Valid loss:  0.04613645\n",
      "17700 Train loss:  0.001177299 Valid loss:  0.0461372\n",
      "17800 Train loss:  0.0011724089 Valid loss:  0.046138454\n",
      "17900 Train loss:  0.001167542 Valid loss:  0.04614028\n",
      "18000 Train loss:  0.0011627027 Valid loss:  0.04614242\n",
      "18100 Train loss:  0.0011578838 Valid loss:  0.0461448\n",
      "18200 Train loss:  0.0011531144 Valid loss:  0.04614695\n",
      "18300 Train loss:  0.0011484239 Valid loss:  0.046148293\n",
      "18400 Train loss:  0.001143874 Valid loss:  0.046148024\n",
      "18500 Train loss:  0.0011395853 Valid loss:  0.046144955\n",
      "18600 Train loss:  0.0011355955 Valid loss:  0.046138532\n",
      "18700 Train loss:  0.0011315151 Valid loss:  0.04613246\n",
      "18800 Train loss:  0.0011268152 Valid loss:  0.046133183\n",
      "18900 Train loss:  0.0011217791 Valid loss:  0.046138983\n",
      "19000 Train loss:  0.0011167319 Valid loss:  0.04614523\n",
      "19100 Train loss:  0.0011117872 Valid loss:  0.046150427\n",
      "19200 Train loss:  0.0011069471 Valid loss:  0.046154562\n",
      "19300 Train loss:  0.0011021956 Valid loss:  0.046158012\n",
      "19400 Train loss:  0.0010975115 Valid loss:  0.046160933\n",
      "19500 Train loss:  0.0010928776 Valid loss:  0.046163484\n",
      "19600 Train loss:  0.0010882864 Valid loss:  0.04616583\n",
      "19700 Train loss:  0.0010837405 Valid loss:  0.046168026\n",
      "19800 Train loss:  0.0010792423 Valid loss:  0.04617015\n",
      "19900 Train loss:  0.0010747849 Valid loss:  0.04617233\n",
      "20000 Train loss:  0.0010703781 Valid loss:  0.04617457\n",
      "20100 Train loss:  0.001066009 Valid loss:  0.046176974\n",
      "20200 Train loss:  0.0010616936 Valid loss:  0.04617952\n",
      "20300 Train loss:  0.001057415 Valid loss:  0.046182264\n",
      "20400 Train loss:  0.0010531741 Valid loss:  0.04618524\n",
      "20500 Train loss:  0.0010489736 Valid loss:  0.0461885\n",
      "20600 Train loss:  0.0010448102 Valid loss:  0.04619203\n",
      "20700 Train loss:  0.0010406763 Valid loss:  0.046195883\n",
      "20800 Train loss:  0.0010365777 Valid loss:  0.04620006\n",
      "20900 Train loss:  0.0010325103 Valid loss:  0.046204552\n",
      "21000 Train loss:  0.001028473 Valid loss:  0.04620942\n",
      "21100 Train loss:  0.0010244658 Valid loss:  0.046214588\n",
      "21200 Train loss:  0.0010204936 Valid loss:  0.04622017\n",
      "21300 Train loss:  0.0010165476 Valid loss:  0.046226136\n",
      "21400 Train loss:  0.0010126357 Valid loss:  0.046232488\n",
      "21500 Train loss:  0.0010087588 Valid loss:  0.046239175\n",
      "21600 Train loss:  0.001004909 Valid loss:  0.046246234\n",
      "21700 Train loss:  0.0010010967 Valid loss:  0.046253644\n",
      "21800 Train loss:  0.000997306 Valid loss:  0.046261404\n",
      "21900 Train loss:  0.000993549 Valid loss:  0.046269435\n",
      "22000 Train loss:  0.0009898157 Valid loss:  0.04627781\n",
      "22100 Train loss:  0.0009861109 Valid loss:  0.046286453\n",
      "22200 Train loss:  0.0009824315 Valid loss:  0.046295445\n",
      "22300 Train loss:  0.0009787744 Valid loss:  0.04630465\n",
      "22400 Train loss:  0.0009751458 Valid loss:  0.04631418\n",
      "22500 Train loss:  0.0009715387 Valid loss:  0.046323948\n",
      "22600 Train loss:  0.00096796075 Valid loss:  0.046333984\n",
      "22700 Train loss:  0.0009644101 Valid loss:  0.046344172\n",
      "22800 Train loss:  0.0009608947 Valid loss:  0.04635448\n",
      "22900 Train loss:  0.00095741055 Valid loss:  0.046364866\n",
      "23000 Train loss:  0.0009539752 Valid loss:  0.046375155\n",
      "23100 Train loss:  0.0009505676 Valid loss:  0.046385333\n",
      "23200 Train loss:  0.0009471822 Valid loss:  0.04639545\n",
      "23300 Train loss:  0.00094382005 Valid loss:  0.046405423\n",
      "23400 Train loss:  0.00094046764 Valid loss:  0.046415314\n",
      "23500 Train loss:  0.00093714223 Valid loss:  0.04642519\n",
      "23600 Train loss:  0.0009338541 Valid loss:  0.04643509\n",
      "23700 Train loss:  0.00093060685 Valid loss:  0.04644517\n",
      "23800 Train loss:  0.000927401 Valid loss:  0.046455417\n",
      "23900 Train loss:  0.0009242066 Valid loss:  0.046466112\n",
      "24000 Train loss:  0.00092103967 Valid loss:  0.046477135\n",
      "24100 Train loss:  0.00091789535 Valid loss:  0.04648839\n",
      "24200 Train loss:  0.0009147777 Valid loss:  0.046499874\n",
      "24300 Train loss:  0.0009116826 Valid loss:  0.046511535\n",
      "24400 Train loss:  0.00090860133 Valid loss:  0.046523444\n",
      "24500 Train loss:  0.00090555294 Valid loss:  0.046535406\n",
      "24600 Train loss:  0.0009025262 Valid loss:  0.04654753\n",
      "24700 Train loss:  0.0008995128 Valid loss:  0.046559762\n",
      "24800 Train loss:  0.00089652085 Valid loss:  0.046572134\n",
      "24900 Train loss:  0.0008935502 Valid loss:  0.046584565\n",
      "25000 Train loss:  0.0008905993 Valid loss:  0.04659716\n",
      "25100 Train loss:  0.0008876657 Valid loss:  0.046609834\n",
      "25200 Train loss:  0.0008847447 Valid loss:  0.046622634\n",
      "25300 Train loss:  0.0008818425 Valid loss:  0.046635617\n",
      "25400 Train loss:  0.0008789501 Valid loss:  0.046648767\n",
      "25500 Train loss:  0.0008760659 Valid loss:  0.046662234\n",
      "25600 Train loss:  0.0008731781 Valid loss:  0.046675954\n",
      "25700 Train loss:  0.0008702747 Valid loss:  0.046690222\n",
      "25800 Train loss:  0.0008673507 Valid loss:  0.046704967\n",
      "25900 Train loss:  0.0008644254 Valid loss:  0.046719994\n",
      "26000 Train loss:  0.00086157944 Valid loss:  0.046734508\n",
      "26100 Train loss:  0.00085884356 Valid loss:  0.046747915\n",
      "26200 Train loss:  0.0008561725 Valid loss:  0.046760596\n",
      "26300 Train loss:  0.0008535196 Valid loss:  0.04677292\n",
      "26400 Train loss:  0.00085088453 Valid loss:  0.046785016\n",
      "26500 Train loss:  0.0008482648 Valid loss:  0.046797097\n",
      "26600 Train loss:  0.0007928087 Valid loss:  0.04735594\n",
      "26700 Train loss:  0.0007903591 Valid loss:  0.047356352\n",
      "26800 Train loss:  0.0007880579 Valid loss:  0.047360044\n",
      "26900 Train loss:  0.0007858186 Valid loss:  0.04736611\n",
      "27000 Train loss:  0.0007836233 Valid loss:  0.047373574\n",
      "27100 Train loss:  0.00078145723 Valid loss:  0.04738184\n",
      "27200 Train loss:  0.0007793169 Valid loss:  0.04739056\n",
      "27300 Train loss:  0.00077719614 Valid loss:  0.047399558\n",
      "27400 Train loss:  0.00077509444 Valid loss:  0.047408745\n",
      "27500 Train loss:  0.00077301037 Valid loss:  0.04741809\n",
      "27600 Train loss:  0.0007709433 Valid loss:  0.047427416\n",
      "27700 Train loss:  0.00076889026 Valid loss:  0.047436833\n",
      "27800 Train loss:  0.0007668536 Valid loss:  0.04744629\n",
      "27900 Train loss:  0.000764831 Valid loss:  0.047455724\n",
      "28000 Train loss:  0.0007628232 Valid loss:  0.047465175\n",
      "28100 Train loss:  0.00076082844 Valid loss:  0.047474544\n",
      "28200 Train loss:  0.0007588465 Valid loss:  0.04748398\n",
      "28300 Train loss:  0.00075687736 Valid loss:  0.04749337\n",
      "28400 Train loss:  0.0007549204 Valid loss:  0.047502715\n",
      "28500 Train loss:  0.0007529746 Valid loss:  0.047512017\n",
      "28600 Train loss:  0.00075104344 Valid loss:  0.04752138\n",
      "28700 Train loss:  0.00074912084 Valid loss:  0.04753068\n",
      "28800 Train loss:  0.0007472104 Valid loss:  0.047539923\n",
      "28900 Train loss:  0.00074530975 Valid loss:  0.047549207\n",
      "29000 Train loss:  0.00074342056 Valid loss:  0.047558457\n",
      "29100 Train loss:  0.0007415424 Valid loss:  0.0475677\n",
      "29200 Train loss:  0.0007396736 Valid loss:  0.047576938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29300 Train loss:  0.0007378149 Valid loss:  0.047586147\n",
      "29400 Train loss:  0.0007359659 Valid loss:  0.04759527\n",
      "29500 Train loss:  0.00073412666 Valid loss:  0.047604486\n",
      "29600 Train loss:  0.0007322972 Valid loss:  0.047613643\n",
      "29700 Train loss:  0.00073047786 Valid loss:  0.04762285\n",
      "29800 Train loss:  0.00072866736 Valid loss:  0.04763207\n",
      "29900 Train loss:  0.00072686566 Valid loss:  0.047641184\n",
      "30000 Train loss:  0.00072507426 Valid loss:  0.047650322\n",
      "30100 Train loss:  0.0007232909 Valid loss:  0.047659513\n",
      "30200 Train loss:  0.00072151783 Valid loss:  0.04766871\n",
      "30300 Train loss:  0.0007197519 Valid loss:  0.047677863\n",
      "30400 Train loss:  0.00071799575 Valid loss:  0.04768711\n",
      "30500 Train loss:  0.00071624824 Valid loss:  0.047696363\n",
      "30600 Train loss:  0.00071450986 Valid loss:  0.04770557\n",
      "30700 Train loss:  0.00071277906 Valid loss:  0.047714747\n",
      "30800 Train loss:  0.00071105687 Valid loss:  0.047723982\n",
      "30900 Train loss:  0.0007093437 Valid loss:  0.047733247\n",
      "31000 Train loss:  0.0007076392 Valid loss:  0.047742568\n",
      "31100 Train loss:  0.00070594193 Valid loss:  0.047751863\n",
      "31200 Train loss:  0.00070425397 Valid loss:  0.047761176\n",
      "31300 Train loss:  0.00070257153 Valid loss:  0.047770478\n",
      "31400 Train loss:  0.00070089847 Valid loss:  0.047779858\n",
      "31500 Train loss:  0.00069923303 Valid loss:  0.04778919\n",
      "31600 Train loss:  0.00069757586 Valid loss:  0.047798593\n",
      "31700 Train loss:  0.0006959263 Valid loss:  0.04780805\n",
      "31800 Train loss:  0.0006942836 Valid loss:  0.047817618\n",
      "31900 Train loss:  0.0006926495 Valid loss:  0.047827147\n",
      "32000 Train loss:  0.0006910227 Valid loss:  0.047836695\n",
      "32100 Train loss:  0.0006894023 Valid loss:  0.047846317\n",
      "32200 Train loss:  0.00068778894 Valid loss:  0.047855962\n",
      "32300 Train loss:  0.000686184 Valid loss:  0.047865637\n",
      "32400 Train loss:  0.0006845862 Valid loss:  0.04787536\n",
      "32500 Train loss:  0.0006829947 Valid loss:  0.047885086\n",
      "32600 Train loss:  0.0006814101 Valid loss:  0.047894914\n",
      "32700 Train loss:  0.0006798329 Valid loss:  0.04790472\n",
      "32800 Train loss:  0.0006782627 Valid loss:  0.047914576\n",
      "32900 Train loss:  0.00067669834 Valid loss:  0.047924537\n",
      "33000 Train loss:  0.0006751409 Valid loss:  0.047934514\n",
      "33100 Train loss:  0.0006735903 Valid loss:  0.047944516\n",
      "33200 Train loss:  0.00067204627 Valid loss:  0.047954537\n",
      "33300 Train loss:  0.0006705094 Valid loss:  0.0479646\n",
      "33400 Train loss:  0.0006689785 Valid loss:  0.04797465\n",
      "33500 Train loss:  0.000667455 Valid loss:  0.04798474\n",
      "33600 Train loss:  0.00066593726 Valid loss:  0.047994774\n",
      "33700 Train loss:  0.0006644261 Valid loss:  0.048004847\n",
      "33800 Train loss:  0.00066292204 Valid loss:  0.048014924\n",
      "33900 Train loss:  0.0006614236 Valid loss:  0.04802504\n",
      "34000 Train loss:  0.0006599325 Valid loss:  0.04803513\n",
      "34100 Train loss:  0.000658448 Valid loss:  0.04804528\n",
      "34200 Train loss:  0.0006569699 Valid loss:  0.048055444\n",
      "34300 Train loss:  0.000655497 Valid loss:  0.048065603\n",
      "34400 Train loss:  0.0006540304 Valid loss:  0.048075818\n",
      "34500 Train loss:  0.0006525712 Valid loss:  0.048085988\n",
      "34600 Train loss:  0.0006511183 Valid loss:  0.048096254\n",
      "34700 Train loss:  0.0006496711 Valid loss:  0.048106514\n",
      "34800 Train loss:  0.00064822973 Valid loss:  0.048116863\n",
      "34900 Train loss:  0.00064679445 Valid loss:  0.048127197\n",
      "35000 Train loss:  0.0006453657 Valid loss:  0.048137516\n",
      "35100 Train loss:  0.000643943 Valid loss:  0.048147857\n",
      "35200 Train loss:  0.00064252585 Valid loss:  0.048158195\n",
      "35300 Train loss:  0.0006411154 Valid loss:  0.048168592\n",
      "35400 Train loss:  0.0006397096 Valid loss:  0.04817897\n",
      "35500 Train loss:  0.0006383105 Valid loss:  0.048189435\n",
      "35600 Train loss:  0.0006369179 Valid loss:  0.048199877\n",
      "35700 Train loss:  0.0006355298 Valid loss:  0.04821039\n",
      "35800 Train loss:  0.000634148 Valid loss:  0.04822093\n",
      "35900 Train loss:  0.0006327707 Valid loss:  0.048231486\n",
      "36000 Train loss:  0.00063140056 Valid loss:  0.048242074\n",
      "36100 Train loss:  0.00063003495 Valid loss:  0.048252665\n",
      "36200 Train loss:  0.0006286764 Valid loss:  0.048263237\n",
      "36300 Train loss:  0.00062732206 Valid loss:  0.048273858\n",
      "36400 Train loss:  0.000625974 Valid loss:  0.048284505\n",
      "36500 Train loss:  0.0006246319 Valid loss:  0.04829521\n",
      "36600 Train loss:  0.00062329386 Valid loss:  0.0483059\n",
      "36700 Train loss:  0.0006219616 Valid loss:  0.048316628\n",
      "36800 Train loss:  0.00062063517 Valid loss:  0.04832735\n",
      "36900 Train loss:  0.0006193132 Valid loss:  0.048338145\n",
      "37000 Train loss:  0.0006179981 Valid loss:  0.048348967\n",
      "37100 Train loss:  0.0006166868 Valid loss:  0.048359755\n",
      "37200 Train loss:  0.00061538094 Valid loss:  0.048370577\n",
      "37300 Train loss:  0.0006140813 Valid loss:  0.048381407\n",
      "37400 Train loss:  0.0006127865 Valid loss:  0.048392195\n",
      "37500 Train loss:  0.0006114966 Valid loss:  0.04840306\n",
      "37600 Train loss:  0.0006102134 Valid loss:  0.048413906\n",
      "37700 Train loss:  0.0006089332 Valid loss:  0.04842478\n",
      "37800 Train loss:  0.00060765905 Valid loss:  0.048435654\n",
      "37900 Train loss:  0.0006063903 Valid loss:  0.048446584\n",
      "38000 Train loss:  0.00060512585 Valid loss:  0.048457522\n",
      "38100 Train loss:  0.0006038676 Valid loss:  0.048468452\n",
      "38200 Train loss:  0.00060261344 Valid loss:  0.048479356\n",
      "38300 Train loss:  0.0006013649 Valid loss:  0.04849031\n",
      "38400 Train loss:  0.00060012034 Valid loss:  0.048501275\n",
      "38500 Train loss:  0.00059888157 Valid loss:  0.04851224\n",
      "38600 Train loss:  0.00059764803 Valid loss:  0.0485232\n",
      "38700 Train loss:  0.0005964186 Valid loss:  0.048534222\n",
      "38800 Train loss:  0.00059519534 Valid loss:  0.048545238\n",
      "38900 Train loss:  0.00059397554 Valid loss:  0.048556205\n",
      "39000 Train loss:  0.00059276127 Valid loss:  0.04856718\n",
      "39100 Train loss:  0.00059155165 Valid loss:  0.048578225\n",
      "39200 Train loss:  0.00059034716 Valid loss:  0.048589222\n",
      "39300 Train loss:  0.0005891474 Valid loss:  0.048600208\n",
      "39400 Train loss:  0.00058795273 Valid loss:  0.048611242\n",
      "39500 Train loss:  0.00058676215 Valid loss:  0.048622295\n",
      "39600 Train loss:  0.0005855768 Valid loss:  0.04863339\n",
      "39700 Train loss:  0.00058439624 Valid loss:  0.048644453\n",
      "39800 Train loss:  0.0005832196 Valid loss:  0.048655536\n",
      "39900 Train loss:  0.0005820481 Valid loss:  0.04866663\n",
      "40000 Train loss:  0.0005808814 Valid loss:  0.048677802\n",
      "40100 Train loss:  0.0005797193 Valid loss:  0.048688922\n",
      "40200 Train loss:  0.0005785612 Valid loss:  0.048700035\n",
      "40300 Train loss:  0.0005774084 Valid loss:  0.04871111\n",
      "40400 Train loss:  0.00057625957 Valid loss:  0.048722208\n",
      "40500 Train loss:  0.0005751148 Valid loss:  0.048733335\n",
      "40600 Train loss:  0.000573976 Valid loss:  0.048744414\n",
      "40700 Train loss:  0.0005728403 Valid loss:  0.048755553\n",
      "40800 Train loss:  0.00057170953 Valid loss:  0.048766643\n",
      "40900 Train loss:  0.000570584 Valid loss:  0.04877779\n",
      "41000 Train loss:  0.00056946214 Valid loss:  0.048788838\n",
      "41100 Train loss:  0.00056834496 Valid loss:  0.048800033\n",
      "41200 Train loss:  0.0005672322 Valid loss:  0.048811123\n",
      "41300 Train loss:  0.0005661231 Valid loss:  0.04882222\n",
      "41400 Train loss:  0.00056501804 Valid loss:  0.0488333\n",
      "41500 Train loss:  0.0005639179 Valid loss:  0.04884443\n",
      "41600 Train loss:  0.00056282204 Valid loss:  0.04885543\n",
      "41700 Train loss:  0.00056173 Valid loss:  0.048866563\n",
      "41800 Train loss:  0.00056064164 Valid loss:  0.04887755\n",
      "41900 Train loss:  0.00055955775 Valid loss:  0.048888583\n",
      "42000 Train loss:  0.00055847934 Valid loss:  0.04889953\n",
      "42100 Train loss:  0.00055740384 Valid loss:  0.048910525\n",
      "42200 Train loss:  0.0005563316 Valid loss:  0.048921514\n",
      "42300 Train loss:  0.0005552645 Valid loss:  0.048932467\n",
      "42400 Train loss:  0.0005542012 Valid loss:  0.04894337\n",
      "42500 Train loss:  0.00055314164 Valid loss:  0.04895429\n",
      "42600 Train loss:  0.0005520859 Valid loss:  0.048965205\n",
      "42700 Train loss:  0.0005510348 Valid loss:  0.04897622\n",
      "42800 Train loss:  0.00054998754 Valid loss:  0.048987135\n",
      "42900 Train loss:  0.0005489437 Valid loss:  0.048998017\n",
      "43000 Train loss:  0.00054790365 Valid loss:  0.049008902\n",
      "43100 Train loss:  0.00054686755 Valid loss:  0.049019743\n",
      "43200 Train loss:  0.0005458359 Valid loss:  0.049030647\n",
      "43300 Train loss:  0.00054480723 Valid loss:  0.049041484\n",
      "43400 Train loss:  0.0005437823 Valid loss:  0.04905232\n",
      "43500 Train loss:  0.00054276123 Valid loss:  0.049063105\n",
      "43600 Train loss:  0.00054174406 Valid loss:  0.04907389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43700 Train loss:  0.0005407303 Valid loss:  0.049084764\n",
      "43800 Train loss:  0.0005397207 Valid loss:  0.049095504\n",
      "43900 Train loss:  0.0005387142 Valid loss:  0.0491063\n",
      "44000 Train loss:  0.00053771085 Valid loss:  0.049117092\n",
      "44100 Train loss:  0.0005367124 Valid loss:  0.04912786\n",
      "44200 Train loss:  0.0005357172 Valid loss:  0.049138617\n",
      "44300 Train loss:  0.0005347249 Valid loss:  0.049149383\n",
      "44400 Train loss:  0.0005337362 Valid loss:  0.049160197\n",
      "44500 Train loss:  0.0005327519 Valid loss:  0.049170956\n",
      "44600 Train loss:  0.0005317701 Valid loss:  0.04918171\n",
      "44700 Train loss:  0.0005307922 Valid loss:  0.049192544\n",
      "44800 Train loss:  0.0005298178 Valid loss:  0.04920334\n",
      "44900 Train loss:  0.0005288463 Valid loss:  0.04921419\n",
      "45000 Train loss:  0.00052787864 Valid loss:  0.04922502\n",
      "45100 Train loss:  0.00052691385 Valid loss:  0.049235947\n",
      "45200 Train loss:  0.0005259525 Valid loss:  0.049246766\n",
      "45300 Train loss:  0.00052499474 Valid loss:  0.049257677\n",
      "45400 Train loss:  0.0005240401 Valid loss:  0.049268566\n",
      "45500 Train loss:  0.00052308856 Valid loss:  0.049279563\n",
      "45600 Train loss:  0.0005221401 Valid loss:  0.049290575\n",
      "45700 Train loss:  0.0005211949 Valid loss:  0.049301587\n",
      "45800 Train loss:  0.0005202524 Valid loss:  0.049312625\n",
      "45900 Train loss:  0.0005193131 Valid loss:  0.049323678\n",
      "46000 Train loss:  0.0005183772 Valid loss:  0.049334735\n",
      "46100 Train loss:  0.0005174437 Valid loss:  0.04934586\n",
      "46200 Train loss:  0.0005165127 Valid loss:  0.04935719\n",
      "46300 Train loss:  0.0005155856 Valid loss:  0.049368653\n",
      "46400 Train loss:  0.00051466114 Valid loss:  0.049380574\n",
      "46500 Train loss:  0.0005137404 Valid loss:  0.049393557\n",
      "46600 Train loss:  0.0005128295 Valid loss:  0.049409814\n",
      "46700 Train loss:  0.00051199 Valid loss:  0.04943709\n",
      "46800 Train loss:  0.0005119384 Valid loss:  0.049504653\n",
      "46900 Train loss:  0.0005194229 Valid loss:  0.04969335\n",
      "47000 Train loss:  0.00052476523 Valid loss:  0.049784727\n",
      "47100 Train loss:  0.00051468296 Valid loss:  0.04967367\n",
      "47200 Train loss:  0.0005112312 Valid loss:  0.04963647\n",
      "47300 Train loss:  0.0005103569 Valid loss:  0.049648385\n",
      "47400 Train loss:  0.00051067024 Valid loss:  0.049684342\n",
      "47500 Train loss:  0.0005111742 Valid loss:  0.04972115\n",
      "47600 Train loss:  0.0005106087 Valid loss:  0.0497385\n",
      "47700 Train loss:  0.0005092498 Valid loss:  0.049742363\n",
      "47800 Train loss:  0.00050803326 Valid loss:  0.049748324\n",
      "47900 Train loss:  0.00050723733 Valid loss:  0.049761526\n",
      "48000 Train loss:  0.0005066813 Valid loss:  0.04977887\n",
      "48100 Train loss:  0.0005060924 Valid loss:  0.04979546\n",
      "48200 Train loss:  0.0005053517 Valid loss:  0.04980943\n",
      "48300 Train loss:  0.00050453073 Valid loss:  0.04982195\n",
      "48400 Train loss:  0.0005037424 Valid loss:  0.049834915\n",
      "48500 Train loss:  0.000503008 Valid loss:  0.049848672\n",
      "48600 Train loss:  0.00050229864 Valid loss:  0.049862783\n",
      "48700 Train loss:  0.00050157594 Valid loss:  0.049876537\n",
      "48800 Train loss:  0.0005008338 Valid loss:  0.049889944\n",
      "48900 Train loss:  0.0005000883 Valid loss:  0.049903225\n",
      "49000 Train loss:  0.0004993584 Valid loss:  0.04991664\n",
      "49100 Train loss:  0.00049863587 Valid loss:  0.0499301\n",
      "49200 Train loss:  0.0004979122 Valid loss:  0.0499435\n",
      "49300 Train loss:  0.0004971915 Valid loss:  0.049956825\n",
      "49400 Train loss:  0.0004964647 Valid loss:  0.049969986\n",
      "49500 Train loss:  0.0004957494 Valid loss:  0.04998322\n",
      "49600 Train loss:  0.00049504 Valid loss:  0.049996506\n",
      "49700 Train loss:  0.0004943318 Valid loss:  0.05000971\n",
      "49800 Train loss:  0.00049362413 Valid loss:  0.050022893\n",
      "49900 Train loss:  0.0004929189 Valid loss:  0.050036006\n",
      "50000 Train loss:  0.0004922167 Valid loss:  0.050049078\n",
      "50100 Train loss:  0.0004915175 Valid loss:  0.05006213\n",
      "50200 Train loss:  0.0004908223 Valid loss:  0.050075244\n",
      "50300 Train loss:  0.0004901297 Valid loss:  0.050088275\n",
      "50400 Train loss:  0.0004894387 Valid loss:  0.050101243\n",
      "50500 Train loss:  0.0004887501 Valid loss:  0.050114196\n",
      "50600 Train loss:  0.00048806265 Valid loss:  0.050127134\n",
      "50700 Train loss:  0.00048737327 Valid loss:  0.05013997\n",
      "50800 Train loss:  0.00048669009 Valid loss:  0.050152805\n",
      "50900 Train loss:  0.00048601071 Valid loss:  0.050165687\n",
      "51000 Train loss:  0.0004853289 Valid loss:  0.050178446\n",
      "51100 Train loss:  0.00048464953 Valid loss:  0.05019122\n",
      "51200 Train loss:  0.000483973 Valid loss:  0.05020397\n",
      "51300 Train loss:  0.0004833 Valid loss:  0.050216727\n",
      "51400 Train loss:  0.00048262384 Valid loss:  0.050229378\n",
      "51500 Train loss:  0.00048195012 Valid loss:  0.05024205\n",
      "51600 Train loss:  0.0004812785 Valid loss:  0.05025471\n",
      "51700 Train loss:  0.00048061335 Valid loss:  0.0502674\n",
      "51800 Train loss:  0.00047994478 Valid loss:  0.050280027\n",
      "51900 Train loss:  0.00047927766 Valid loss:  0.050292697\n",
      "52000 Train loss:  0.00047861246 Valid loss:  0.050305232\n",
      "52100 Train loss:  0.0004779496 Valid loss:  0.050317835\n",
      "52200 Train loss:  0.0004772868 Valid loss:  0.050330408\n",
      "52300 Train loss:  0.0004766238 Valid loss:  0.050342914\n",
      "52400 Train loss:  0.0004759693 Valid loss:  0.05035553\n",
      "52500 Train loss:  0.0004753124 Valid loss:  0.050368097\n",
      "52600 Train loss:  0.00047465027 Valid loss:  0.050380517\n",
      "52700 Train loss:  0.00047399485 Valid loss:  0.050393026\n",
      "52800 Train loss:  0.0004733408 Valid loss:  0.050405476\n",
      "52900 Train loss:  0.00047268847 Valid loss:  0.050417937\n",
      "53000 Train loss:  0.00047203782 Valid loss:  0.05043041\n",
      "53100 Train loss:  0.00047138485 Valid loss:  0.050442826\n",
      "53200 Train loss:  0.0004707391 Valid loss:  0.050455313\n",
      "53300 Train loss:  0.0004700911 Valid loss:  0.050467774\n",
      "53400 Train loss:  0.00046944682 Valid loss:  0.05048023\n",
      "53500 Train loss:  0.00046879836 Valid loss:  0.050492562\n",
      "53600 Train loss:  0.0004681568 Valid loss:  0.05050504\n",
      "53700 Train loss:  0.00046751366 Valid loss:  0.050517436\n",
      "53800 Train loss:  0.00046687503 Valid loss:  0.05052985\n",
      "53900 Train loss:  0.00046622887 Valid loss:  0.05054218\n",
      "54000 Train loss:  0.0004655915 Valid loss:  0.050554603\n",
      "54100 Train loss:  0.00046495703 Valid loss:  0.050567\n",
      "54200 Train loss:  0.0004643211 Valid loss:  0.05057937\n",
      "54300 Train loss:  0.00046368816 Valid loss:  0.05059175\n",
      "54400 Train loss:  0.00046305373 Valid loss:  0.050604064\n",
      "54500 Train loss:  0.0004624183 Valid loss:  0.05061635\n",
      "54600 Train loss:  0.00046179077 Valid loss:  0.050628737\n",
      "54700 Train loss:  0.00046116216 Valid loss:  0.050641052\n",
      "54800 Train loss:  0.00046053666 Valid loss:  0.050653383\n",
      "54900 Train loss:  0.0004599058 Valid loss:  0.050665613\n",
      "55000 Train loss:  0.00045927876 Valid loss:  0.050677847\n",
      "55100 Train loss:  0.00045865326 Valid loss:  0.050690133\n",
      "55200 Train loss:  0.00045803734 Valid loss:  0.05070249\n",
      "55300 Train loss:  0.000457416 Valid loss:  0.050714727\n",
      "55400 Train loss:  0.00045679318 Valid loss:  0.05072692\n",
      "55500 Train loss:  0.0004561699 Valid loss:  0.05073907\n",
      "55600 Train loss:  0.00045555233 Valid loss:  0.05075131\n",
      "55700 Train loss:  0.00045493877 Valid loss:  0.050763518\n",
      "55800 Train loss:  0.0004543221 Valid loss:  0.05077569\n",
      "55900 Train loss:  0.00045370663 Valid loss:  0.050787825\n",
      "56000 Train loss:  0.00045308954 Valid loss:  0.05079993\n",
      "56100 Train loss:  0.00045247446 Valid loss:  0.05081196\n",
      "56200 Train loss:  0.0004518616 Valid loss:  0.050824035\n",
      "56300 Train loss:  0.0004512502 Valid loss:  0.050836053\n",
      "56400 Train loss:  0.00045063414 Valid loss:  0.050848003\n",
      "56500 Train loss:  0.0004500224 Valid loss:  0.05085997\n",
      "56600 Train loss:  0.00043565736 Valid loss:  0.05053648\n",
      "56700 Train loss:  0.00043513466 Valid loss:  0.050543774\n",
      "56800 Train loss:  0.0004346225 Valid loss:  0.050551128\n",
      "56900 Train loss:  0.00043411588 Valid loss:  0.050558664\n",
      "57000 Train loss:  0.00043361387 Valid loss:  0.050566263\n",
      "57100 Train loss:  0.00043311395 Valid loss:  0.050573934\n",
      "57200 Train loss:  0.0004326166 Valid loss:  0.050581638\n",
      "57300 Train loss:  0.00043212177 Valid loss:  0.050589446\n",
      "57400 Train loss:  0.000431629 Valid loss:  0.05059729\n",
      "57500 Train loss:  0.00043113763 Valid loss:  0.050605156\n",
      "57600 Train loss:  0.00043064778 Valid loss:  0.05061302\n",
      "57700 Train loss:  0.00043016017 Valid loss:  0.050621\n",
      "57800 Train loss:  0.00042967327 Valid loss:  0.050628986\n",
      "57900 Train loss:  0.00042918843 Valid loss:  0.050636977\n",
      "58000 Train loss:  0.00042870422 Valid loss:  0.05064497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58100 Train loss:  0.00042822186 Valid loss:  0.05065303\n",
      "58200 Train loss:  0.00042774077 Valid loss:  0.050661013\n",
      "58300 Train loss:  0.0004272605 Valid loss:  0.050668977\n",
      "58400 Train loss:  0.0004267815 Valid loss:  0.05067693\n",
      "58500 Train loss:  0.00042630438 Valid loss:  0.050684877\n",
      "58600 Train loss:  0.0004258276 Valid loss:  0.050692786\n",
      "58700 Train loss:  0.00042535225 Valid loss:  0.050700665\n",
      "58800 Train loss:  0.00042487806 Valid loss:  0.05070853\n",
      "58900 Train loss:  0.0004244052 Valid loss:  0.050716355\n",
      "59000 Train loss:  0.0004239337 Valid loss:  0.050724164\n",
      "59100 Train loss:  0.00042346295 Valid loss:  0.050731916\n",
      "59200 Train loss:  0.0004229932 Valid loss:  0.050739694\n",
      "59300 Train loss:  0.0004225247 Valid loss:  0.050747413\n",
      "59400 Train loss:  0.00042205732 Valid loss:  0.050755113\n",
      "59500 Train loss:  0.00042159043 Valid loss:  0.050762773\n",
      "59600 Train loss:  0.00042112486 Valid loss:  0.05077038\n",
      "59700 Train loss:  0.0004206609 Valid loss:  0.050777953\n",
      "59800 Train loss:  0.0004201969 Valid loss:  0.050785504\n",
      "59900 Train loss:  0.00041973472 Valid loss:  0.050793033\n",
      "60000 Train loss:  0.00041927354 Valid loss:  0.050800517\n",
      "60100 Train loss:  0.00041881276 Valid loss:  0.05080791\n",
      "60200 Train loss:  0.00041835324 Valid loss:  0.0508153\n",
      "60300 Train loss:  0.00041789433 Valid loss:  0.05082259\n",
      "60400 Train loss:  0.00041743662 Valid loss:  0.050829876\n",
      "60500 Train loss:  0.00041697963 Valid loss:  0.050837077\n",
      "60600 Train loss:  0.00041652395 Valid loss:  0.05084425\n",
      "60700 Train loss:  0.00041606888 Valid loss:  0.05085137\n",
      "60800 Train loss:  0.00041561495 Valid loss:  0.05085844\n",
      "60900 Train loss:  0.00041516143 Valid loss:  0.050865453\n",
      "61000 Train loss:  0.00041470892 Valid loss:  0.05087244\n",
      "61100 Train loss:  0.00041425784 Valid loss:  0.050879378\n",
      "61200 Train loss:  0.00041380676 Valid loss:  0.050886244\n",
      "61300 Train loss:  0.00041335696 Valid loss:  0.05089309\n",
      "61400 Train loss:  0.00041290832 Valid loss:  0.05089994\n",
      "61500 Train loss:  0.0004124605 Valid loss:  0.05090674\n",
      "61600 Train loss:  0.00041201373 Valid loss:  0.050913524\n",
      "61700 Train loss:  0.00041156771 Valid loss:  0.050920293\n",
      "61800 Train loss:  0.0004111223 Valid loss:  0.050927065\n",
      "61900 Train loss:  0.0004106784 Valid loss:  0.05093379\n",
      "62000 Train loss:  0.00041023528 Valid loss:  0.05094051\n",
      "62100 Train loss:  0.00040979288 Valid loss:  0.05094722\n",
      "62200 Train loss:  0.00040935163 Valid loss:  0.050953887\n",
      "62300 Train loss:  0.00040891083 Valid loss:  0.0509606\n",
      "62400 Train loss:  0.0004084716 Valid loss:  0.050967216\n",
      "62500 Train loss:  0.00040803218 Valid loss:  0.050973896\n",
      "62600 Train loss:  0.0004075948 Valid loss:  0.050980583\n",
      "62700 Train loss:  0.0004071583 Valid loss:  0.050987266\n",
      "62800 Train loss:  0.00040672297 Valid loss:  0.050993953\n",
      "62900 Train loss:  0.00040628779 Valid loss:  0.051000617\n",
      "63000 Train loss:  0.00040585376 Valid loss:  0.05100732\n",
      "63100 Train loss:  0.00040542096 Valid loss:  0.051014088\n",
      "63200 Train loss:  0.0004049887 Valid loss:  0.05102078\n",
      "63300 Train loss:  0.00040455782 Valid loss:  0.05102752\n",
      "63400 Train loss:  0.00040412738 Valid loss:  0.0510343\n",
      "63500 Train loss:  0.00040369807 Valid loss:  0.05104106\n",
      "63600 Train loss:  0.00040326975 Valid loss:  0.051047843\n",
      "63700 Train loss:  0.00040284224 Valid loss:  0.051054694\n",
      "63800 Train loss:  0.0004024151 Valid loss:  0.051061563\n",
      "63900 Train loss:  0.00040198962 Valid loss:  0.05106847\n",
      "64000 Train loss:  0.0004015648 Valid loss:  0.05107534\n",
      "64100 Train loss:  0.00040114025 Valid loss:  0.05108227\n",
      "64200 Train loss:  0.00040071743 Valid loss:  0.051089197\n",
      "64300 Train loss:  0.0004002951 Valid loss:  0.051096193\n",
      "64400 Train loss:  0.00039987377 Valid loss:  0.051103197\n",
      "64500 Train loss:  0.00039945275 Valid loss:  0.051110223\n",
      "64600 Train loss:  0.00039903304 Valid loss:  0.05111727\n",
      "64700 Train loss:  0.00039861447 Valid loss:  0.05112436\n",
      "64800 Train loss:  0.0003981962 Valid loss:  0.051131442\n",
      "64900 Train loss:  0.00039777913 Valid loss:  0.05113852\n",
      "65000 Train loss:  0.00039736292 Valid loss:  0.05114567\n",
      "65100 Train loss:  0.00039694709 Valid loss:  0.051152814\n",
      "65200 Train loss:  0.00039653268 Valid loss:  0.051159978\n",
      "65300 Train loss:  0.00039611902 Valid loss:  0.05116717\n",
      "65400 Train loss:  0.00039570619 Valid loss:  0.05117437\n",
      "65500 Train loss:  0.00039529408 Valid loss:  0.0511816\n",
      "65600 Train loss:  0.00039488295 Valid loss:  0.051188868\n",
      "65700 Train loss:  0.00039447244 Valid loss:  0.051196203\n",
      "65800 Train loss:  0.00039406316 Valid loss:  0.051203527\n",
      "65900 Train loss:  0.0003936543 Valid loss:  0.05121088\n",
      "66000 Train loss:  0.00039324653 Valid loss:  0.051218268\n",
      "66100 Train loss:  0.00039283905 Valid loss:  0.05122563\n",
      "66200 Train loss:  0.00039243232 Valid loss:  0.05123308\n",
      "66300 Train loss:  0.00039202688 Valid loss:  0.051240563\n",
      "66400 Train loss:  0.00039162236 Valid loss:  0.051248055\n",
      "66500 Train loss:  0.00039121835 Valid loss:  0.05125563\n",
      "66600 Train loss:  0.00039081523 Valid loss:  0.051263154\n",
      "66700 Train loss:  0.0003904128 Valid loss:  0.051270716\n",
      "66800 Train loss:  0.00039001156 Valid loss:  0.05127834\n",
      "66900 Train loss:  0.00038961068 Valid loss:  0.051285975\n",
      "67000 Train loss:  0.00038921065 Valid loss:  0.05129369\n",
      "67100 Train loss:  0.00038881146 Valid loss:  0.051301394\n",
      "67200 Train loss:  0.00038841274 Valid loss:  0.05130914\n",
      "67300 Train loss:  0.00038801515 Valid loss:  0.05131687\n",
      "67400 Train loss:  0.00038761823 Valid loss:  0.05132466\n",
      "67500 Train loss:  0.00038722204 Valid loss:  0.05133241\n",
      "67600 Train loss:  0.0003868266 Valid loss:  0.05134022\n",
      "67700 Train loss:  0.00038643208 Valid loss:  0.051348012\n",
      "67800 Train loss:  0.0003860381 Valid loss:  0.051355854\n",
      "67900 Train loss:  0.00038564487 Valid loss:  0.051363703\n",
      "68000 Train loss:  0.0003852529 Valid loss:  0.051371608\n",
      "68100 Train loss:  0.00038486125 Valid loss:  0.051379524\n",
      "68200 Train loss:  0.00038447033 Valid loss:  0.05138747\n",
      "68300 Train loss:  0.00038408 Valid loss:  0.05139536\n",
      "68400 Train loss:  0.0003836909 Valid loss:  0.051403344\n",
      "68500 Train loss:  0.00038330213 Valid loss:  0.051411305\n",
      "68600 Train loss:  0.00038291406 Valid loss:  0.05141931\n",
      "68700 Train loss:  0.0003825269 Valid loss:  0.051427327\n",
      "68800 Train loss:  0.00038214028 Valid loss:  0.051435374\n",
      "68900 Train loss:  0.00038175457 Valid loss:  0.051443424\n",
      "69000 Train loss:  0.00038136917 Valid loss:  0.051451523\n",
      "69100 Train loss:  0.0003809854 Valid loss:  0.051459596\n",
      "69200 Train loss:  0.00038060118 Valid loss:  0.05146772\n",
      "69300 Train loss:  0.00038021812 Valid loss:  0.05147586\n",
      "69400 Train loss:  0.00037983595 Valid loss:  0.05148402\n",
      "69500 Train loss:  0.00037945478 Valid loss:  0.051492214\n",
      "69600 Train loss:  0.00037907358 Valid loss:  0.051500376\n",
      "69700 Train loss:  0.0003786933 Valid loss:  0.051508583\n",
      "69800 Train loss:  0.00037831446 Valid loss:  0.05151684\n",
      "69900 Train loss:  0.00037793518 Valid loss:  0.051525094\n",
      "70000 Train loss:  0.00037755692 Valid loss:  0.05153338\n",
      "70100 Train loss:  0.00037717936 Valid loss:  0.051541682\n",
      "70200 Train loss:  0.0003768026 Valid loss:  0.05155004\n",
      "70300 Train loss:  0.0003764262 Valid loss:  0.051558383\n",
      "70400 Train loss:  0.0003760503 Valid loss:  0.051566724\n",
      "70500 Train loss:  0.00037567527 Valid loss:  0.05157508\n",
      "70600 Train loss:  0.00037530053 Valid loss:  0.051583413\n",
      "70700 Train loss:  0.00037492655 Valid loss:  0.051591784\n",
      "70800 Train loss:  0.00037455314 Valid loss:  0.051600147\n",
      "70900 Train loss:  0.00037418047 Valid loss:  0.051608555\n",
      "71000 Train loss:  0.00037380858 Valid loss:  0.051616926\n",
      "71100 Train loss:  0.0003734366 Valid loss:  0.051625315\n",
      "71200 Train loss:  0.00037306512 Valid loss:  0.05163375\n",
      "71300 Train loss:  0.00037269443 Valid loss:  0.05164219\n",
      "71400 Train loss:  0.00037232452 Valid loss:  0.05165062\n",
      "71500 Train loss:  0.00037195414 Valid loss:  0.05165911\n",
      "71600 Train loss:  0.00037158528 Valid loss:  0.05166761\n",
      "71700 Train loss:  0.00037121648 Valid loss:  0.051676113\n",
      "71800 Train loss:  0.00037084837 Valid loss:  0.051684644\n",
      "71900 Train loss:  0.00037048032 Valid loss:  0.051693108\n",
      "72000 Train loss:  0.00037011236 Valid loss:  0.05170164\n",
      "72100 Train loss:  0.0003697452 Valid loss:  0.051710177\n",
      "72200 Train loss:  0.00036937877 Valid loss:  0.051718716\n",
      "72300 Train loss:  0.00036901265 Valid loss:  0.051727246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72400 Train loss:  0.00036864725 Valid loss:  0.05173585\n",
      "72500 Train loss:  0.00036828217 Valid loss:  0.05174438\n",
      "72600 Train loss:  0.00036791762 Valid loss:  0.05175298\n",
      "72700 Train loss:  0.0003675539 Valid loss:  0.051761568\n",
      "72800 Train loss:  0.00036719046 Valid loss:  0.051770147\n",
      "72900 Train loss:  0.00036682808 Valid loss:  0.05177876\n",
      "73000 Train loss:  0.00036646676 Valid loss:  0.051787384\n",
      "73100 Train loss:  0.00036610576 Valid loss:  0.05179606\n",
      "73200 Train loss:  0.00036574595 Valid loss:  0.051804792\n",
      "73300 Train loss:  0.00036538666 Valid loss:  0.051813457\n",
      "73400 Train loss:  0.0003650288 Valid loss:  0.05182221\n",
      "73500 Train loss:  0.00036467178 Valid loss:  0.051831007\n",
      "73600 Train loss:  0.0003643152 Valid loss:  0.051839802\n",
      "73700 Train loss:  0.00036396008 Valid loss:  0.051848605\n",
      "73800 Train loss:  0.00036360574 Valid loss:  0.051857367\n",
      "73900 Train loss:  0.0003632521 Valid loss:  0.051866174\n",
      "74000 Train loss:  0.0003628994 Valid loss:  0.051875\n",
      "74100 Train loss:  0.0003625479 Valid loss:  0.051883847\n",
      "74200 Train loss:  0.00036219682 Valid loss:  0.05189276\n",
      "74300 Train loss:  0.0003618466 Valid loss:  0.051901583\n",
      "74400 Train loss:  0.00036149714 Valid loss:  0.05191051\n",
      "74500 Train loss:  0.000361148 Valid loss:  0.051919423\n",
      "74600 Train loss:  0.00036079987 Valid loss:  0.051928375\n",
      "74700 Train loss:  0.0003604533 Valid loss:  0.051937312\n",
      "74800 Train loss:  0.00036010638 Valid loss:  0.05194631\n",
      "74900 Train loss:  0.0003597607 Valid loss:  0.051955294\n",
      "75000 Train loss:  0.00035941537 Valid loss:  0.051964305\n",
      "75100 Train loss:  0.00035907104 Valid loss:  0.051973317\n",
      "75200 Train loss:  0.00035872718 Valid loss:  0.05198232\n",
      "75300 Train loss:  0.00035838384 Valid loss:  0.051991325\n",
      "75400 Train loss:  0.00035804132 Valid loss:  0.05200036\n",
      "75500 Train loss:  0.0003576992 Valid loss:  0.05200942\n",
      "75600 Train loss:  0.00035735828 Valid loss:  0.05201843\n",
      "75700 Train loss:  0.00035701794 Valid loss:  0.052027516\n",
      "75800 Train loss:  0.000356678 Valid loss:  0.052036565\n",
      "75900 Train loss:  0.00035633892 Valid loss:  0.05204567\n",
      "76000 Train loss:  0.00035599997 Valid loss:  0.05205472\n",
      "76100 Train loss:  0.0003556622 Valid loss:  0.052063845\n",
      "76200 Train loss:  0.0003553247 Valid loss:  0.052072898\n",
      "76300 Train loss:  0.00035498774 Valid loss:  0.052081984\n",
      "76400 Train loss:  0.0003546517 Valid loss:  0.052091084\n",
      "76500 Train loss:  0.00035431611 Valid loss:  0.052100185\n",
      "76600 Train loss:  0.00035398125 Valid loss:  0.052109286\n",
      "76700 Train loss:  0.00035364687 Valid loss:  0.052118376\n",
      "76800 Train loss:  0.00035331363 Valid loss:  0.052127462\n",
      "76900 Train loss:  0.00035298037 Valid loss:  0.052136596\n",
      "77000 Train loss:  0.0003526479 Valid loss:  0.052145723\n",
      "77100 Train loss:  0.0003523165 Valid loss:  0.052154895\n",
      "77200 Train loss:  0.00035198487 Valid loss:  0.05216402\n",
      "77300 Train loss:  0.00035165457 Valid loss:  0.052173205\n",
      "77400 Train loss:  0.00035132447 Valid loss:  0.052182388\n",
      "77500 Train loss:  0.0003509953 Valid loss:  0.052191556\n",
      "77600 Train loss:  0.0003506667 Valid loss:  0.052200783\n",
      "77700 Train loss:  0.00035033902 Valid loss:  0.052209985\n",
      "77800 Train loss:  0.000350011 Valid loss:  0.052219227\n",
      "77900 Train loss:  0.0003496841 Valid loss:  0.05222852\n",
      "78000 Train loss:  0.000349358 Valid loss:  0.05223775\n",
      "78100 Train loss:  0.00034903226 Valid loss:  0.052247036\n",
      "78200 Train loss:  0.00034870702 Valid loss:  0.052256335\n",
      "78300 Train loss:  0.0003483824 Valid loss:  0.05226562\n",
      "78400 Train loss:  0.00034805873 Valid loss:  0.052274924\n",
      "78500 Train loss:  0.00034773545 Valid loss:  0.052284293\n",
      "78600 Train loss:  0.00034741257 Valid loss:  0.052293576\n",
      "78700 Train loss:  0.0003470905 Valid loss:  0.052302923\n",
      "78800 Train loss:  0.00034676894 Valid loss:  0.052312247\n",
      "78900 Train loss:  0.00034644778 Valid loss:  0.0523216\n",
      "79000 Train loss:  0.0003461272 Valid loss:  0.05233091\n",
      "79100 Train loss:  0.0003458075 Valid loss:  0.05234028\n",
      "79200 Train loss:  0.00034548843 Valid loss:  0.05234966\n",
      "79300 Train loss:  0.00034516997 Valid loss:  0.052359004\n",
      "79400 Train loss:  0.0003448515 Valid loss:  0.052368395\n",
      "79500 Train loss:  0.00034453385 Valid loss:  0.052377753\n",
      "79600 Train loss:  0.00034421668 Valid loss:  0.052387137\n",
      "79700 Train loss:  0.0003439004 Valid loss:  0.0523965\n",
      "79800 Train loss:  0.00034358408 Valid loss:  0.052405883\n",
      "79900 Train loss:  0.00034326912 Valid loss:  0.05241528\n",
      "80000 Train loss:  0.00034295412 Valid loss:  0.052424707\n",
      "80100 Train loss:  0.00034263998 Valid loss:  0.05243415\n",
      "80200 Train loss:  0.0003423261 Valid loss:  0.052443624\n",
      "80300 Train loss:  0.00034201297 Valid loss:  0.052453034\n",
      "80400 Train loss:  0.00034170048 Valid loss:  0.05246251\n",
      "80500 Train loss:  0.00034138837 Valid loss:  0.052471966\n",
      "80600 Train loss:  0.0003410766 Valid loss:  0.052481487\n",
      "80700 Train loss:  0.00034076555 Valid loss:  0.052490957\n",
      "80800 Train loss:  0.00034045495 Valid loss:  0.052500527\n",
      "80900 Train loss:  0.00034014523 Valid loss:  0.052509993\n",
      "81000 Train loss:  0.0003398357 Valid loss:  0.05251951\n",
      "81100 Train loss:  0.00033952665 Valid loss:  0.05252907\n",
      "81200 Train loss:  0.00033921882 Valid loss:  0.052538596\n",
      "81300 Train loss:  0.0003389113 Valid loss:  0.05254815\n",
      "81400 Train loss:  0.00033860424 Valid loss:  0.05255774\n",
      "81500 Train loss:  0.0003382973 Valid loss:  0.052567285\n",
      "81600 Train loss:  0.00033799087 Valid loss:  0.052576862\n",
      "81700 Train loss:  0.00033768537 Valid loss:  0.05258646\n",
      "81800 Train loss:  0.00033738054 Valid loss:  0.052596074\n",
      "81900 Train loss:  0.00033707597 Valid loss:  0.052605666\n",
      "82000 Train loss:  0.0003367718 Valid loss:  0.05261525\n",
      "82100 Train loss:  0.0003364682 Valid loss:  0.05262484\n",
      "82200 Train loss:  0.0003361658 Valid loss:  0.05263446\n",
      "82300 Train loss:  0.00033586312 Valid loss:  0.052644126\n",
      "82400 Train loss:  0.00033556097 Valid loss:  0.052653763\n",
      "82500 Train loss:  0.0003352597 Valid loss:  0.052663427\n",
      "82600 Train loss:  0.00033495927 Valid loss:  0.05267311\n",
      "82700 Train loss:  0.00033465872 Valid loss:  0.052682776\n",
      "82800 Train loss:  0.00033435854 Valid loss:  0.052692443\n",
      "82900 Train loss:  0.0003340591 Valid loss:  0.052702114\n",
      "83000 Train loss:  0.0003337605 Valid loss:  0.05271182\n",
      "83100 Train loss:  0.00033346185 Valid loss:  0.05272152\n",
      "83200 Train loss:  0.00033316406 Valid loss:  0.05273122\n",
      "83300 Train loss:  0.00033286677 Valid loss:  0.052740913\n",
      "83400 Train loss:  0.0003325697 Valid loss:  0.05275061\n",
      "83500 Train loss:  0.00033227346 Valid loss:  0.052760296\n",
      "83600 Train loss:  0.00033197762 Valid loss:  0.052769966\n",
      "83700 Train loss:  0.0003316823 Valid loss:  0.052779693\n",
      "83800 Train loss:  0.00033138788 Valid loss:  0.05278945\n",
      "83900 Train loss:  0.0003310934 Valid loss:  0.052799158\n",
      "84000 Train loss:  0.0003307994 Valid loss:  0.0528089\n",
      "84100 Train loss:  0.00033050583 Valid loss:  0.052818596\n",
      "84200 Train loss:  0.00033021282 Valid loss:  0.052828375\n",
      "84300 Train loss:  0.0003299205 Valid loss:  0.052838136\n",
      "84400 Train loss:  0.00032962876 Valid loss:  0.052847873\n",
      "84500 Train loss:  0.00032933697 Valid loss:  0.05285767\n",
      "84600 Train loss:  0.00032904654 Valid loss:  0.052867416\n",
      "84700 Train loss:  0.0003287558 Valid loss:  0.052877206\n",
      "84800 Train loss:  0.00032846586 Valid loss:  0.05288702\n",
      "84900 Train loss:  0.00032817645 Valid loss:  0.052896786\n",
      "85000 Train loss:  0.00032788812 Valid loss:  0.052906573\n",
      "85100 Train loss:  0.00032759894 Valid loss:  0.05291636\n",
      "85200 Train loss:  0.00032731117 Valid loss:  0.052926186\n",
      "85300 Train loss:  0.00032702376 Valid loss:  0.052935977\n",
      "85400 Train loss:  0.00032673674 Valid loss:  0.052945763\n",
      "85500 Train loss:  0.00032645007 Valid loss:  0.05295559\n",
      "85600 Train loss:  0.00032616386 Valid loss:  0.05296541\n",
      "85700 Train loss:  0.0003258785 Valid loss:  0.052975208\n",
      "85800 Train loss:  0.00032559337 Valid loss:  0.052985054\n",
      "85900 Train loss:  0.00032530844 Valid loss:  0.0529949\n",
      "86000 Train loss:  0.00032502442 Valid loss:  0.053004723\n",
      "86100 Train loss:  0.00032474071 Valid loss:  0.053014625\n",
      "86200 Train loss:  0.00032445756 Valid loss:  0.053024486\n",
      "86300 Train loss:  0.00032417456 Valid loss:  0.053034335\n",
      "86400 Train loss:  0.00032389245 Valid loss:  0.05304426\n",
      "86500 Train loss:  0.00032361035 Valid loss:  0.053054128\n",
      "86600 Train loss:  0.0003233286 Valid loss:  0.053064015\n",
      "86700 Train loss:  0.00032304798 Valid loss:  0.05307394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86800 Train loss:  0.0003227675 Valid loss:  0.05308379\n",
      "86900 Train loss:  0.0003224874 Valid loss:  0.053093664\n",
      "87000 Train loss:  0.0003222077 Valid loss:  0.05310358\n",
      "87100 Train loss:  0.0003219284 Valid loss:  0.053113475\n",
      "87200 Train loss:  0.00032165006 Valid loss:  0.05312335\n",
      "87300 Train loss:  0.0003213718 Valid loss:  0.05313324\n",
      "87400 Train loss:  0.00032109383 Valid loss:  0.05314314\n",
      "87500 Train loss:  0.00032081647 Valid loss:  0.05315303\n",
      "87600 Train loss:  0.00032054 Valid loss:  0.053162944\n",
      "87700 Train loss:  0.00032026364 Valid loss:  0.05317282\n",
      "87800 Train loss:  0.0003199877 Valid loss:  0.053182762\n",
      "87900 Train loss:  0.0003197123 Valid loss:  0.05319261\n",
      "88000 Train loss:  0.00031943718 Valid loss:  0.053202584\n",
      "88100 Train loss:  0.00031916282 Valid loss:  0.053212535\n",
      "88200 Train loss:  0.00031888846 Valid loss:  0.053222455\n",
      "88300 Train loss:  0.0003186147 Valid loss:  0.05323243\n",
      "88400 Train loss:  0.00031834142 Valid loss:  0.053242378\n",
      "88500 Train loss:  0.00031806887 Valid loss:  0.05325235\n",
      "88600 Train loss:  0.00031779648 Valid loss:  0.053262364\n",
      "88700 Train loss:  0.00031752448 Valid loss:  0.053272318\n",
      "88800 Train loss:  0.00031725303 Valid loss:  0.053282324\n",
      "88900 Train loss:  0.00031698227 Valid loss:  0.05329235\n",
      "89000 Train loss:  0.00031671187 Valid loss:  0.053302385\n",
      "89100 Train loss:  0.00031644164 Valid loss:  0.053312395\n",
      "89200 Train loss:  0.00031617182 Valid loss:  0.05332242\n",
      "89300 Train loss:  0.00031590238 Valid loss:  0.053332437\n",
      "89400 Train loss:  0.00031563372 Valid loss:  0.053342454\n",
      "89500 Train loss:  0.00031536544 Valid loss:  0.053352483\n",
      "89600 Train loss:  0.00031509748 Valid loss:  0.053362533\n",
      "89700 Train loss:  0.0003148299 Valid loss:  0.05337253\n",
      "89800 Train loss:  0.00031456296 Valid loss:  0.05338259\n",
      "89900 Train loss:  0.00031429625 Valid loss:  0.053392597\n",
      "90000 Train loss:  0.0003140301 Valid loss:  0.053402636\n",
      "90100 Train loss:  0.0003137643 Valid loss:  0.053412676\n",
      "90200 Train loss:  0.00031349921 Valid loss:  0.05342271\n",
      "90300 Train loss:  0.00031323417 Valid loss:  0.053432725\n",
      "90400 Train loss:  0.00031296982 Valid loss:  0.053442735\n",
      "90500 Train loss:  0.0003127056 Valid loss:  0.05345282\n",
      "90600 Train loss:  0.000312442 Valid loss:  0.053462856\n",
      "90700 Train loss:  0.00031217904 Valid loss:  0.053472877\n",
      "90800 Train loss:  0.00031191614 Valid loss:  0.05348294\n",
      "90900 Train loss:  0.00031165374 Valid loss:  0.053492952\n",
      "91000 Train loss:  0.0003113918 Valid loss:  0.053502973\n",
      "91100 Train loss:  0.00031113042 Valid loss:  0.053513046\n",
      "91200 Train loss:  0.00031086936 Valid loss:  0.05352308\n",
      "91300 Train loss:  0.0003106089 Valid loss:  0.053533133\n",
      "91400 Train loss:  0.00031034817 Valid loss:  0.05354318\n",
      "91500 Train loss:  0.00031008862 Valid loss:  0.053553224\n",
      "91600 Train loss:  0.0003098292 Valid loss:  0.053563267\n",
      "91700 Train loss:  0.00030956994 Valid loss:  0.05357338\n",
      "91800 Train loss:  0.00030931138 Valid loss:  0.05358344\n",
      "91900 Train loss:  0.0003090533 Valid loss:  0.05359356\n",
      "92000 Train loss:  0.0003087953 Valid loss:  0.053603705\n",
      "92100 Train loss:  0.00030853826 Valid loss:  0.05361377\n",
      "92200 Train loss:  0.0003082808 Valid loss:  0.053623848\n",
      "92300 Train loss:  0.00030802455 Valid loss:  0.053633947\n",
      "92400 Train loss:  0.00030776847 Valid loss:  0.053644095\n",
      "92500 Train loss:  0.0003075123 Valid loss:  0.05365419\n",
      "92600 Train loss:  0.0003072572 Valid loss:  0.05366429\n",
      "92700 Train loss:  0.00030700228 Valid loss:  0.053674437\n",
      "92800 Train loss:  0.00030674788 Valid loss:  0.053684544\n",
      "92900 Train loss:  0.00030649343 Valid loss:  0.053694658\n",
      "93000 Train loss:  0.0003062394 Valid loss:  0.05370478\n",
      "93100 Train loss:  0.00030598597 Valid loss:  0.053714927\n",
      "93200 Train loss:  0.00030573312 Valid loss:  0.053725038\n",
      "93300 Train loss:  0.0003054803 Valid loss:  0.053735208\n",
      "93400 Train loss:  0.0003052284 Valid loss:  0.053745307\n",
      "93500 Train loss:  0.00030497665 Valid loss:  0.053755436\n",
      "93600 Train loss:  0.0003047253 Valid loss:  0.05376556\n",
      "93700 Train loss:  0.0003044739 Valid loss:  0.05377565\n",
      "93800 Train loss:  0.00030422333 Valid loss:  0.053785805\n",
      "93900 Train loss:  0.0003039729 Valid loss:  0.053795874\n",
      "94000 Train loss:  0.00030372335 Valid loss:  0.053806007\n",
      "94100 Train loss:  0.00030347376 Valid loss:  0.053816143\n",
      "94200 Train loss:  0.00030322451 Valid loss:  0.053826224\n",
      "94300 Train loss:  0.00030297582 Valid loss:  0.05383634\n",
      "94400 Train loss:  0.00030272757 Valid loss:  0.05384645\n",
      "94500 Train loss:  0.00030247957 Valid loss:  0.05385652\n",
      "94600 Train loss:  0.0003022321 Valid loss:  0.053866666\n",
      "94700 Train loss:  0.00030198492 Valid loss:  0.0538768\n",
      "94800 Train loss:  0.0003017382 Valid loss:  0.053886954\n",
      "94900 Train loss:  0.00030149193 Valid loss:  0.053897087\n",
      "95000 Train loss:  0.0003012459 Valid loss:  0.053907204\n",
      "95100 Train loss:  0.0003010004 Valid loss:  0.053917296\n",
      "95200 Train loss:  0.00030075558 Valid loss:  0.053927395\n",
      "95300 Train loss:  0.0003005103 Valid loss:  0.053937517\n",
      "95400 Train loss:  0.000300266 Valid loss:  0.05394763\n",
      "95500 Train loss:  0.0003000219 Valid loss:  0.053957753\n",
      "95600 Train loss:  0.00029977807 Valid loss:  0.053967863\n",
      "95700 Train loss:  0.00029953485 Valid loss:  0.053977985\n",
      "95800 Train loss:  0.0002992916 Valid loss:  0.053988032\n",
      "95900 Train loss:  0.00029904975 Valid loss:  0.053998124\n",
      "96000 Train loss:  0.0002988072 Valid loss:  0.054008238\n",
      "96100 Train loss:  0.00029856552 Valid loss:  0.054018263\n",
      "96200 Train loss:  0.0002983241 Valid loss:  0.054028366\n",
      "96300 Train loss:  0.0002980828 Valid loss:  0.05403846\n",
      "96400 Train loss:  0.000297842 Valid loss:  0.05404849\n",
      "96500 Train loss:  0.0002976019 Valid loss:  0.054058533\n",
      "96600 Train loss:  0.00029736172 Valid loss:  0.05406862\n",
      "96700 Train loss:  0.00029712226 Valid loss:  0.05407863\n",
      "96800 Train loss:  0.00029688326 Valid loss:  0.054088686\n",
      "96900 Train loss:  0.0002966444 Valid loss:  0.054098744\n",
      "97000 Train loss:  0.00029640572 Valid loss:  0.05410883\n",
      "97100 Train loss:  0.00029616768 Valid loss:  0.054118887\n",
      "97200 Train loss:  0.00029593002 Valid loss:  0.05412894\n",
      "97300 Train loss:  0.00029569297 Valid loss:  0.054138947\n",
      "97400 Train loss:  0.00029545557 Valid loss:  0.05414902\n",
      "97500 Train loss:  0.00029521884 Valid loss:  0.054159082\n",
      "97600 Train loss:  0.0002949826 Valid loss:  0.054169178\n",
      "97700 Train loss:  0.00029474695 Valid loss:  0.054179218\n",
      "97800 Train loss:  0.0002945112 Valid loss:  0.05418929\n",
      "97900 Train loss:  0.00029427587 Valid loss:  0.054199345\n",
      "98000 Train loss:  0.00029404115 Valid loss:  0.0542094\n",
      "98100 Train loss:  0.00029380654 Valid loss:  0.05421946\n",
      "98200 Train loss:  0.0002935728 Valid loss:  0.054229487\n",
      "98300 Train loss:  0.00029333908 Valid loss:  0.054239556\n",
      "98400 Train loss:  0.0002931056 Valid loss:  0.054249633\n",
      "98500 Train loss:  0.0002928723 Valid loss:  0.05425967\n",
      "98600 Train loss:  0.00029264003 Valid loss:  0.05426972\n",
      "98700 Train loss:  0.00029240755 Valid loss:  0.054279752\n",
      "98800 Train loss:  0.00029217527 Valid loss:  0.054289818\n",
      "98900 Train loss:  0.00029194387 Valid loss:  0.054299872\n",
      "99000 Train loss:  0.0002917126 Valid loss:  0.05430988\n",
      "99100 Train loss:  0.00029148185 Valid loss:  0.054319926\n",
      "99200 Train loss:  0.00029125132 Valid loss:  0.054329947\n",
      "99300 Train loss:  0.00029102096 Valid loss:  0.05434002\n",
      "99400 Train loss:  0.00029079086 Valid loss:  0.054350022\n",
      "99500 Train loss:  0.00029056135 Valid loss:  0.054360125\n",
      "99600 Train loss:  0.0002903318 Valid loss:  0.054370113\n",
      "99700 Train loss:  0.00029010314 Valid loss:  0.054380193\n",
      "99800 Train loss:  0.00028987453 Valid loss:  0.054390214\n",
      "99900 Train loss:  0.0002896464 Valid loss:  0.05440025\n"
     ]
    }
   ],
   "source": [
    "# Reads the potential training data & sets up a neural network w/ 2 hidden layers\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "bins = 128\n",
    "seedmax = 1 # opens seed files 0 - 19\n",
    "trainx = []\n",
    "trainy = []\n",
    "validx = []\n",
    "validy = []\n",
    "\n",
    "#python needs pointers\n",
    "for k in range(seedmax):\n",
    "    with open('test_pots'+str(k)+'.csv', 'r') as csvfile:\n",
    "        flurg = csv.reader(csvfile)\n",
    "        for row in flurg:\n",
    "            trainx.append([float(num) for num in row])\n",
    "    with open('test_out'+str(k)+'.csv', 'r') as csvfile:\n",
    "        flurg = csv.reader(csvfile)\n",
    "        for row in flurg:\n",
    "            trainy.append([float(num) for num in row])\n",
    "    with open('valid_pots'+str(k)+'.csv', 'r') as csvfile:\n",
    "        flurg = csv.reader(csvfile)\n",
    "        for row in flurg:\n",
    "            validx.append([float(num) for num in row])\n",
    "    with open('valid_out'+str(k)+'.csv', 'r') as csvfile:\n",
    "        flurg = csv.reader(csvfile)\n",
    "        for row in flurg:\n",
    "            validy.append([float(num) for num in row])\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "#have a decaying learning rate so that convergence is faster at first and the fit is better at the end.\n",
    "#However, by trial and error, the simple exponential decay doesn't work well.\n",
    "#Trying a method by which the decay happens at hand-specified intervals\n",
    "startrate = 0.125\n",
    "gs = 0\n",
    "gslist = [1,1,2,3,10,20,40,100,200,10000]\n",
    "ic = 0\n",
    "learnrate = tf.Variable(startrate, trainable=False)\n",
    "updatelearnrate = tf.assign(learnrate,tf.multiply(learnrate,0.75))\n",
    "# set up neural network layers. There are shorter ways to do it, but this exposes the guts.\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "#1st hidden layer\n",
    "W1 = tf.Variable(tf.random_uniform([bins-1, bins-1], -1./bins, 1./bins))\n",
    "B1 = tf.Variable(tf.random_uniform([bins-1], -1., 1.))\n",
    "L1 = tf.nn.softplus(tf.matmul(X, W1) + B1)\n",
    "#2nd hidden layer\n",
    "W2 = tf.Variable(tf.random_uniform([bins-1, bins-1], -1./bins, 1./bins))\n",
    "B2 = tf.Variable(tf.random_uniform([bins-1], -1., 1.))\n",
    "L2 = tf.nn.softplus(tf.matmul(L1, W2) + B2)\n",
    "#Output layer\n",
    "W3 = tf.Variable(tf.random_uniform([bins-1, bins-1], -1./bins, 1./bins))\n",
    "B3 = tf.Variable(tf.random_uniform([bins-1], -1., 1.))\n",
    "L3 = tf.nn.softplus(tf.matmul(L2, W3) + B3)\n",
    "#Cost function\n",
    "costfunc = tf.reduce_mean(tf.square(tf.subtract(L3,Y)))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learnrate)\n",
    "trainstep = optimizer.minimize(costfunc)\n",
    "#initialize\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(100000):\n",
    "    if step % 150 == 0:\n",
    "        if ic == gslist[gs]:\n",
    "            gs = gs + 1\n",
    "            ic = 1\n",
    "            sess.run(updatelearnrate)\n",
    "        else:\n",
    "            ic = ic + 1\n",
    "    if step %100 == 0:\n",
    "        print step, 'Train loss: ',sess.run(costfunc,feed_dict={X: trainx, Y: trainy}), 'Valid loss: ',sess.run(costfunc,feed_dict={X: validx, Y: validy})\n",
    "    sess.run(trainstep, feed_dict={X: trainx, Y: trainy})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubler(aray):\n",
    "    dbled = np.zeros([2*i for i in aray.shape])\n",
    "    if len(aray.shape) == 1:\n",
    "        for i in range(aray.shape[0]):\n",
    "            dbled[2*i] = aray[i]\n",
    "            dbled[2*i+1] = aray[i]\n",
    "    elif len(aray.shape) == 2:\n",
    "        for i in range(aray.shape[0]):\n",
    "            for j in range(aray.shape[1]):\n",
    "                dbled[2*i][2*j] = aray[i][j]\n",
    "                dbled[2*i+1][2*j] = aray[i][j]\n",
    "                dbled[2*i][2*j+1] = aray[i][j]\n",
    "                dbled[2*i+1][2*j+1] = aray[i][j]\n",
    "    return dbled\n",
    "\n",
    "from PIL import Image\n",
    "we1 = sess.run(W1)\n",
    "bi1 = sess.run(B1)\n",
    "we2 = sess.run(W2)\n",
    "bi2 = sess.run(B2)\n",
    "we3 = sess.run(W3)\n",
    "bi3 = sess.run(B3)\n",
    "\n",
    "gauswid = 1.\n",
    "weiscale = []\n",
    "for i in range(bins-1):\n",
    "    line = np.exp([-np.square(float(i-j)/gauswid)/2. for j in range(bins-1)])\n",
    "    line = np.divide(line,sum(line))\n",
    "    weiscale.append(line.tolist())\n",
    "\n",
    "weconv1 = np.matmul(we1,weiscale)\n",
    "weconv2 = np.matmul(we2,weiscale)\n",
    "\n",
    "sign = 1\n",
    "mask = np.zeros(bins-1)\n",
    "for i in range(bins-1):\n",
    "    ind = (bins-2)/2+int(np.floor((i+1)/2))*sign\n",
    "    sign = -sign\n",
    "    mxin = np.argmax(np.add(weconv1[ind],mask))\n",
    "    swapper = np.identity(bins-1)\n",
    "    swapper[ind][ind] = 0\n",
    "    swapper[mxin][mxin] = 0\n",
    "    swapper[ind][mxin] = 1\n",
    "    swapper[mxin][ind] = 1\n",
    "    we1 = np.matmul(we1,swapper)\n",
    "    weconv1 = np.matmul(weconv1,swapper)\n",
    "    bi1 = np.matmul(bi1,swapper)\n",
    "    we2 = np.matmul(swapper,we2)\n",
    "    mask[ind] = -1.E12\n",
    "\n",
    "sign = 1\n",
    "mask = np.zeros(bins-1)\n",
    "for i in range(bins-1):\n",
    "    ind = (bins-2)/2+int(np.floor((i+1)/2))*sign\n",
    "    sign = -sign\n",
    "    mxin = np.argmax(np.add(weconv2[ind],mask))\n",
    "    swapper = np.identity(bins-1)\n",
    "    swapper[ind][ind] = 0\n",
    "    swapper[mxin][mxin] = 0\n",
    "    swapper[ind][mxin] = 1\n",
    "    swapper[mxin][ind] = 1\n",
    "    we2 = np.matmul(we2,swapper)\n",
    "    weconv2 = np.matmul(weconv2,swapper)\n",
    "    bi2 = np.matmul(bi2,swapper)\n",
    "    we3 = np.matmul(swapper,we3)\n",
    "    mask[ind] = -1.E12\n",
    "\n",
    "\n",
    "max1 = max(max(we1.tolist()))\n",
    "min1 = min(min(we1.tolist()))\n",
    "wedb1 = doubler(we1)\n",
    "weight1 = np.divide(np.subtract(wedb1,min1),max1-min1)\n",
    "wim1 = Image.fromarray((weight1*255).astype(np.uint8),'L')\n",
    "wim1.save('W1.bmp')\n",
    "max1 = max(bi1.tolist())\n",
    "min1 = min(bi1.tolist())\n",
    "bidb1 = doubler(bi1)\n",
    "bia1 = np.divide(np.subtract(bidb1,min1),max1-min1)\n",
    "bias1 = np.array([bia1.tolist() for i in range(32)])\n",
    "bim1 = Image.fromarray((bias1*255).astype(np.uint8),'L')\n",
    "bim1.save('B1.bmp')\n",
    "\n",
    "max2 = max(max(we2.tolist()))\n",
    "min2 = min(min(we2.tolist()))\n",
    "wedb2 = doubler(we2)\n",
    "weight2 = np.divide(np.subtract(wedb2,min2),max2-min2)\n",
    "wim2 = Image.fromarray((weight2*255).astype(np.uint8),'L')\n",
    "wim2.save('W2.bmp')\n",
    "max2 = max(bi2.tolist())\n",
    "min2 = min(bi2.tolist())\n",
    "bidb2 = doubler(bi2)\n",
    "bia2 = np.divide(np.subtract(bidb2,min2),max2-min2)\n",
    "bias2 = np.array([bia2.tolist() for i in range(32)])\n",
    "bim2 = Image.fromarray((bias2*255).astype(np.uint8),'L')\n",
    "bim2.save('B2.bmp')\n",
    "\n",
    "max3 = max(max(we3.tolist()))\n",
    "min3 = min(min(we3.tolist()))\n",
    "wedb3 = doubler(we3)\n",
    "weight3 = np.divide(np.subtract(wedb3,min3),max3-min3)\n",
    "wim3 = Image.fromarray((weight3*255).astype(np.uint8),'L')\n",
    "wim3.save('W3.bmp')\n",
    "max3 = max(bi3.tolist())\n",
    "min3 = min(bi3.tolist())\n",
    "bidb3 = doubler(bi3)\n",
    "bia3 = np.divide(np.subtract(bidb3,min3),max3-min3)\n",
    "bias3 = np.array([bia3.tolist() for i in range(32)])\n",
    "bim3 = Image.fromarray((bias3*255).astype(np.uint8),'L')\n",
    "bim3.save('B3.bmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('W1.csv', 'w') as f:\n",
    "    fileout = csv.writer(f)\n",
    "    fileout.writerows(sess.run(W1).tolist())\n",
    "with open('W2.csv', 'w') as f:\n",
    "    fileout = csv.writer(f)\n",
    "    fileout.writerows(sess.run(W2).tolist())\n",
    "with open('W3.csv', 'w') as f:\n",
    "    fileout = csv.writer(f)\n",
    "    fileout.writerows(sess.run(W3).tolist())\n",
    "with open('B1.csv', 'w') as f:\n",
    "    fileout = csv.writer(f)\n",
    "    fileout.writerows([sess.run(B1).tolist()])\n",
    "with open('B2.csv', 'w') as f:\n",
    "    fileout = csv.writer(f)\n",
    "    fileout.writerows([sess.run(B2).tolist()])\n",
    "with open('B3.csv', 'w') as f:\n",
    "    fileout = csv.writer(f)\n",
    "    fileout.writerows([sess.run(B3).tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuD9ELS5SAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x123dff0d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "9580"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "potenid = 479\n",
    "plt.plot(sess.run(L3,feed_dict={X: [trainx[potenid]]})[0])\n",
    "plt.plot([trainx[potenid][k]/max(trainx[potenid]) for k in range(bins - 1)])\n",
    "plt.plot(trainy[potenid])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
